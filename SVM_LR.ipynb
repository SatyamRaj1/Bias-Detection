{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatyamRaj1/Bias-Detection/blob/main/SVM_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCd_OYNoHSXx",
        "outputId": "22008354-7ffc-4864-eed3-d2d17d2aacd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Data/ToxicBias\")"
      ],
      "metadata": {
        "id": "A4bmu_9roDUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foRnbPKzaMM6"
      },
      "source": [
        "#Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9TxmVVpITdT"
      },
      "outputs": [],
      "source": [
        "# # Initialize an empty dictionary to store the data frames\n",
        "dfs = {}\n",
        "# # Load each CSV file into a Pandas DataFrame and store it in the dictionary with appropriate keys\n",
        "# # Now, you have three data frames: data_frames['train'], data_frames['test'], and data_frames['validation']\n",
        "# # You can access and manipulate these data frames as needed.\n",
        "dfs['train'] = pd.read_csv('toxicbias_train.csv')\n",
        "dfs['test'] = pd.read_csv('toxicbias_test.csv')\n",
        "dfs['validation'] = pd.read_csv('toxicbias_val.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k80DiU_zJxFX",
        "outputId": "dd736a09-3c7c-47d9-d376-0f91ad657e13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['comment_text', 'identity_attack_x', 'bias', 'target', 'category',\n",
              "       'rationale'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dfs['train'].columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXK26lt2Lzu3"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming you have three dataframes: dfs['train'], dfs['test'], and dfs['validation']\n",
        "\n",
        "# Data Preprocessing\n",
        "# Assuming 'bias' is binary (biased or not biased)\n",
        "# Convert 'bias' to 0 (not biased) and 1 (biased)\n",
        "dfs['train']['bias'] = dfs['train']['bias'].apply(lambda x: 1 if x == 'bias' else 0)\n",
        "dfs['test']['bias'] = dfs['test']['bias'].apply(lambda x: 1 if x == 'bias' else 0)\n",
        "dfs['validation']['bias'] = dfs['validation']['bias'].apply(lambda x: 1 if x == 'bias' else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOOn6PymOSwC",
        "outputId": "86448133-addf-48b2-a4eb-a72e57a2b2da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "867"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "count_of_neutral = dfs['train']['bias'].value_counts()[0]\n",
        "count_of_neutral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjK8c6W3aY3f"
      },
      "source": [
        "# Logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9fbS4u3ONmd",
        "outputId": "4b26ec9e-b8ed-4043-dfac-04da182fcf93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Accuracy: 0.8\n",
            "Test Set Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.01      0.02       131\n",
            "           1       0.80      1.00      0.89       519\n",
            "\n",
            "    accuracy                           0.80       650\n",
            "   macro avg       0.90      0.50      0.45       650\n",
            "weighted avg       0.84      0.80      0.71       650\n",
            "\n",
            "Validation Set Accuracy: 0.8009259259259259\n",
            "The comment is not biased.\n"
          ]
        }
      ],
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')  # You can adjust max_features as needed\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(dfs['train']['comment_text'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(dfs['test']['comment_text'])\n",
        "X_validation_tfidf = tfidf_vectorizer.transform(dfs['validation']['comment_text'])\n",
        "\n",
        "# Create and Train the Logistic Regression Model\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_tfidf, dfs['train']['bias'])\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = lr_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = accuracy_score(dfs['test']['bias'], y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n",
        "\n",
        "# You can also print a classification report for more detailed metrics\n",
        "print(\"Test Set Classification Report:\")\n",
        "print(classification_report(dfs['test']['bias'], y_pred))\n",
        "\n",
        "# Now you can use this model to make predictions on the validation set\n",
        "y_val_pred = lr_model.predict(X_validation_tfidf)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_accuracy = accuracy_score(dfs['validation']['bias'], y_val_pred)\n",
        "print(\"Validation Set Accuracy:\", val_accuracy)\n",
        "\n",
        "# To predict the bias of a new comment\n",
        "new_comment = \"This is a biased comment\"\n",
        "new_comment_tfidf = tfidf_vectorizer.transform([new_comment])\n",
        "prediction = lr_model.predict(new_comment_tfidf)\n",
        "if prediction[0] == 1:\n",
        "    print(\"The comment is biased.\")\n",
        "else:\n",
        "    print(\"The comment is not biased.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-eS1AnyPmF6",
        "outputId": "90ab14f2-d38c-4f3c-a888-1a27a68d9d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The comment is biased.\n"
          ]
        }
      ],
      "source": [
        "# To predict the bias of a new comment\n",
        "new_comment = \"This is not a good model\"\n",
        "new_comment_tfidf = tfidf_vectorizer.transform([new_comment])\n",
        "prediction = lr_model.predict(new_comment_tfidf)\n",
        "if prediction[0] == 1:\n",
        "    print(\"The comment is biased.\")\n",
        "else:\n",
        "    print(\"The comment is not biased.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1p1uq8_adXA"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK1_gfqOWdx3",
        "outputId": "8259aaab-6381-4acf-9754-39e1b3072d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Accuracy: 0.8015384615384615\n",
            "Test Set Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.02      0.04       131\n",
            "           1       0.80      1.00      0.89       519\n",
            "\n",
            "    accuracy                           0.80       650\n",
            "   macro avg       0.78      0.51      0.47       650\n",
            "weighted avg       0.79      0.80      0.72       650\n",
            "\n",
            "Validation Set Accuracy: 0.7962962962962963\n",
            "The comment is not biased.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming you have three dataframes: dfs['train'], dfs['test'], and dfs['validation']\n",
        "\n",
        "# Create and Train the SVM Model with Linear Kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_tfidf, dfs['train']['bias'])\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = accuracy_score(dfs['test']['bias'], y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n",
        "\n",
        "# You can also print a classification report for more detailed metrics\n",
        "print(\"Test Set Classification Report:\")\n",
        "print(classification_report(dfs['test']['bias'], y_pred))\n",
        "\n",
        "# Now you can use this model to make predictions on the validation set\n",
        "y_val_pred = svm_model.predict(X_validation_tfidf)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_accuracy = accuracy_score(dfs['validation']['bias'], y_val_pred)\n",
        "print(\"Validation Set Accuracy:\", val_accuracy)\n",
        "\n",
        "# To predict the bias of a new comment\n",
        "new_comment = \"This is a biased comment\"\n",
        "new_comment_tfidf = tfidf_vectorizer.transform([new_comment])\n",
        "prediction = svm_model.predict(new_comment_tfidf)\n",
        "if prediction[0] == 1:\n",
        "    print(\"The comment is biased.\")\n",
        "else:\n",
        "    print(\"The comment is not biased.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7EoDZ1WaiKj"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etNnTld20QEY",
        "outputId": "e66cee67-3f95-4e73-cba7-0a21ecbc5d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.5 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.5\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torchtext==0.6 torch==1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "53u5M-kOnRI7",
        "outputId": "d19436f5-29b1-49e8-c93d-922eb626abf0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b95afebf9908>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTabularDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T16o8fpnZFB"
      },
      "outputs": [],
      "source": [
        "# Assuming you have dfs['train'], dfs['test'], and dfs['validation'] dataframes\n",
        "\n",
        "# Define the Fields for text and labels\n",
        "TEXT = Field(sequential=True, tokenize='spacy', lower=True, include_lengths=True)\n",
        "LABEL = Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# Create TabularDatasets\n",
        "fields = [('comment_text', TEXT), ('bias', LABEL)]\n",
        "train_data, test_data, val_data = TabularDataset.splits(\n",
        "    path='.', train=dfs['train'], validation=dfs['validation'], test=dfs['test'], format='csv', fields=fields)\n",
        "\n",
        "# Build the vocabulary using pre-trained GloVe word vectors\n",
        "TEXT.build_vocab(train_data, vectors=\"glove.6B.300d\")\n",
        "# If you don't have GloVe vectors, you can train them from scratch or download them\n",
        "\n",
        "# Create Iterators to batch the data\n",
        "BATCH_SIZE = 64\n",
        "train_iterator, test_iterator, val_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data, val_data), batch_size=BATCH_SIZE, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
        "        super(LSTMModel, self).__init()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) if self.lstm.bidirectional else hidden[-1, :, :])\n",
        "        return self.fc(hidden)\n",
        "\n",
        "# Define hyperparameters\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1  # Binary classification\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "\n",
        "# Create the model\n",
        "model = LSTMModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
        "\n",
        "# Use pre-trained word embeddings\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Place the model and loss function on the GPU (if available)\n",
        "model = model.to('cuda')\n",
        "criterion = criterion.to('cuda')\n",
        "\n",
        "# Training function\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, text_lengths = batch.comment_text\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, batch.bias.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.comment_text\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            loss = criterion(predictions, batch.bias.float())\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# Training loop\n",
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, val_iterator, criterion)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'lstm-model.pt')  # Save the best model\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {valid_loss:.3f}')\n",
        "\n",
        "# Load the best model and test it\n",
        "model.load_state_dict(torch.load('lstm-model.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "\n",
        "# Make predictions for a new comment\n",
        "def predict_sentiment(model, tokenizer, comment):\n",
        "    model.eval()\n",
        "    tokenized = tokenizer(comment)\n",
        "    indexed = [TEXT.vocab.stoi[x] for x in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to('cuda')\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()\n",
        "\n",
        "new_comment = \"This is a biased comment\"\n",
        "predicted_bias = predict_sentiment(model, TEXT, new_comment)\n",
        "if predicted_bias > 0.5:\n",
        "    print(\"The comment is biased.\")\n",
        "else:\n",
        "    print(\"The comment is not biased.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87mBInDu1c8f"
      },
      "source": [
        "# BERT Heirachal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGoHNCNhlRIl"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zU_M0sk1b5z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. Preprocess the text data and labels\n",
        "# You'll need to tokenize the text and convert labels to one-hot encoding.\n",
        "\n",
        "# 2. Load the BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Load your data\n",
        "train_df = dfs['train']\n",
        "test_df = dfs['test']\n",
        "validation_df = dfs['validation']\n",
        "\n",
        "# Preprocess the text and labels\n",
        "def preprocess_data(df, max_length):\n",
        "    tokenized_text = []\n",
        "    for text in df['comment_text']:\n",
        "        encoded_text = tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True)\n",
        "        tokenized_text.append(encoded_text)\n",
        "\n",
        "    input_ids = torch.tensor([text + [tokenizer.pad_token_id] * (max_length - len(text)) for text in tokenized_text]).to(device)\n",
        "\n",
        "    label_mapping = {'neutral': 0, 'bias': 1}\n",
        "    labels = torch.tensor([label_mapping[label] for label in df['bias']]).to(device)\n",
        "    return input_ids, labels\n",
        "\n",
        "# Set the maximum sequence length\n",
        "max_length = 128\n",
        "\n",
        "# Preprocess the data for training, validation, and test\n",
        "train_input_ids, train_labels = preprocess_data(train_df, max_length)\n",
        "validation_input_ids, validation_labels = preprocess_data(validation_df, max_length)\n",
        "test_input_ids, test_labels = preprocess_data(test_df, max_length)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "\n",
        "train_data = DataLoader(TensorDataset(train_input_ids, train_labels), batch_size=batch_size, shuffle=True)\n",
        "validation_data = DataLoader(TensorDataset(validation_input_ids, validation_labels), batch_size=batch_size)\n",
        "test_data = DataLoader(TensorDataset(test_input_ids, test_labels), batch_size=batch_size)\n",
        "\n",
        "# 4. Define the bias detection and category classification models\n",
        "# You need to create a separate model for category classification.\n",
        "\n",
        "class CategoryClassificationModel(nn.Module):\n",
        "    def __init__(self, num_categories):\n",
        "        super(CategoryClassificationModel, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_categories).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu4JCtcAmik7"
      },
      "outputs": [],
      "source": [
        "# 5. Define loss functions and optimizers\n",
        "bias_detection_criterion = nn.CrossEntropyLoss()\n",
        "category_classification_criterion = nn.BCEWithLogitsLoss()\n",
        "bias_detection_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
        "optimizer = AdamW(bias_detection_model.parameters(), lr=0.001, eps=1e-8)\n",
        "\n",
        "# 6. Train the bias detection model\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_data:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = bias_detection_model(inputs)\n",
        "        loss = bias_detection_criterion(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# 7. Filter the input data using the trained bias detection model\n",
        "filtered_data = []\n",
        "for text, label in test_data:\n",
        "    with torch.no_grad():\n",
        "        outputs = bias_detection_model(text)\n",
        "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
        "        filtered_data.append((text[predicted_labels == 1], label[predicted_labels == 1]))\n",
        "\n",
        "# 8. Create data loaders for category classification\n",
        "category_train_data = DataLoader(category_train_inputs, category_train_labels, batch_size=batch_size, shuffle=True)\n",
        "category_validation_data = DataLoader(category_validation_inputs, category_validation_labels, batch_size=batch_size)\n",
        "\n",
        "# 9. Train the category classification model\n",
        "category_model = CategoryClassificationModel(num_categories).to(device)\n",
        "optimizer_category = AdamW(category_model.parameters(), lr=..., eps=...)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in category_train_data:\n",
        "        inputs, labels = batch\n",
        "        optimizer_category.zero_grad()\n",
        "        outputs = category_model(inputs)\n",
        "        loss = category_classification_criterion(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer_category.step()\n",
        "\n",
        "# Evaluate the hierarchical model on the test dataset\n",
        "category_model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_data:\n",
        "        inputs, labels = batch\n",
        "        outputs = category_model(inputs)\n",
        "        # Evaluate the category classification results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Multitask"
      ],
      "metadata": {
        "id": "qIH_tCu2b4_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "TjTQntx9cgZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load your data\n",
        "train_df = dfs['train']\n",
        "test_df = dfs['test']\n",
        "validation_df = dfs['validation']\n",
        "\n",
        "# Initialize the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)"
      ],
      "metadata": {
        "id": "WhBU6ZfKb393"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the batch size and maximum sequence length\n",
        "batch_size = 16\n",
        "max_seq_length = 128\n",
        "\n",
        "# Define a custom dataset for bias detection and bias category classification\n",
        "class CustomMultiTaskDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Define a mapping of labels to numerical values for bias detection\n",
        "        self.bias_label_map = {\"bias\": 1, \"neutral\": 0}\n",
        "\n",
        "        # Define a mapping of labels to numerical values for bias categories\n",
        "        self.category_label_map = {\"Political\": 0, \"Religion\": 1, \"Gender\": 2, \"LGBTQ\": 3, \"Race\": 4}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Extract text, bias, and category labels from the DataFrame\n",
        "        text = self.data['comment_text'].iloc[idx]\n",
        "        bias_labels = self.bias_label_map[self.data['bias'].iloc[idx]]  # Map bias labels to numerical values\n",
        "\n",
        "        # Map category labels to numerical values\n",
        "        # If the category is not recognized, assign -1 (you can handle this case as needed)\n",
        "        category_labels = self.category_label_map.get(self.data['category'].iloc[idx], -1)\n",
        "\n",
        "        if category_labels == -1:\n",
        "            # Handle unknown categories or 'None' values\n",
        "            # You can choose how to handle these cases, e.g., assign them to a default category or ignore them.\n",
        "            pass\n",
        "\n",
        "        # Tokenize the text and create input tensors\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length,\n",
        "                                  return_tensors='pt', return_attention_mask=True)\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,  # Input IDs for BERT\n",
        "            'attention_mask': attention_mask,  # Attention mask for BERT\n",
        "            'bias_labels': torch.tensor(bias_labels, dtype=torch.long),  # Numerical bias label\n",
        "            'category_labels': torch.tensor(category_labels, dtype=torch.long)  # Numerical category label\n",
        "        }"
      ],
      "metadata": {
        "id": "1BKqw2oDcL8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders for bias detection and bias category classification\n",
        "train_dataset = CustomMultiTaskDataset(train_df, tokenizer, max_seq_length)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "validation_dataset = CustomMultiTaskDataset(validation_df, tokenizer, max_seq_length)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
        "\n",
        "test_dataset = CustomMultiTaskDataset(test_df, tokenizer, max_seq_length)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "hEmKHgC5cZXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a multi-task model with two output heads\n",
        "class MultiTaskBERT(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model_name, num_categories):\n",
        "        super(MultiTaskBERT, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=2)  # Bias detection head\n",
        "        self.category_head = torch.nn.Linear(self.bert.config.hidden_size, num_categories)  # Bias category head\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[0]  # Logits for bias detection\n",
        "\n",
        "        # Extract the [CLS] token embedding for category classification\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
        "        bias_logits = logits\n",
        "        category_logits = self.category_head(cls_embedding)  # Logits for category classification\n",
        "        return bias_logits, category_logits\n",
        "\n",
        "# Create the multi-task model\n",
        "model = MultiTaskBERT(\"bert-base-uncased\", num_categories=5).to(device)"
      ],
      "metadata": {
        "id": "fWQkbDI4hwID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "pJ7A3QRtkdfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, validation_loader, epochs, device):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            bias_labels = batch['bias_labels'].to(device)  # Labels for bias detection\n",
        "            category_labels = batch['category_labels'].to(device)  # Labels for bias category classification\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass through the model\n",
        "            bias_logits, category_logits = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Calculate loss for bias detection\n",
        "            bias_loss = criterion(bias_logits, bias_labels)\n",
        "\n",
        "            # Calculate loss for bias category classification\n",
        "            # Ensure you handle unknown categories appropriately in your data preprocessing\n",
        "            # Here, we ignore unknown categories (-1)\n",
        "            known_category_mask = category_labels != -1\n",
        "            category_loss = criterion(category_logits[known_category_mask], category_labels[known_category_mask])\n",
        "\n",
        "            # Total loss is a combination of bias and category losses\n",
        "            total_loss = bias_loss + category_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        bias_predictions = []\n",
        "        category_predictions = []\n",
        "        true_bias_labels = []\n",
        "        true_category_labels = []\n",
        "\n",
        "        # Validation loop\n",
        "        with torch.no_grad():\n",
        "            for batch in validation_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                bias_labels = batch['bias_labels'].to(device)\n",
        "                category_labels = batch['category_labels'].to(device)\n",
        "                bias_logits, category_logits = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "                # Calculate validation loss for bias detection\n",
        "                val_loss += criterion(bias_logits, bias_labels)\n",
        "\n",
        "                # Store predictions and true labels\n",
        "                bias_predictions.extend(bias_logits.argmax(dim=1).tolist())\n",
        "                category_predictions.extend(category_logits.argmax(dim=1).tolist())\n",
        "                true_bias_labels.extend(bias_labels.tolist())\n",
        "                true_category_labels.extend(category_labels.tolist())\n",
        "\n",
        "        print(f\"Validation Loss: {val_loss / len(validation_loader)}\")\n",
        "\n",
        "        # Calculate accuracy for bias detection\n",
        "        bias_acc = accuracy_score(true_bias_labels, bias_predictions)\n",
        "\n",
        "        # Calculate accuracy for bias category classification\n",
        "        known_category_mask = [category != -1 for category in true_category_labels]\n",
        "        category_acc = accuracy_score([true_category_labels[i] for i, known in enumerate(known_category_mask) if known],\n",
        "                                      [category_predictions[i] for i, known in enumerate(known_category_mask) if known])\n",
        "\n",
        "        # Print the validation accuracy for both bias detection and category classification\n",
        "        print(f\"Validation Bias Accuracy: {bias_acc}\")\n",
        "        print(f\"Validation Category Accuracy: {category_acc}\")\n",
        "        # Additional classification reports or metrics can be added as needed\n",
        "\n"
      ],
      "metadata": {
        "id": "2EDE0cbYcnJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train(model, train_loader, validation_loader, epochs=3, device = device)"
      ],
      "metadata": {
        "id": "knChEnyCcqay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            test_loss += outputs.loss.item()\n",
        "\n",
        "            logits = outputs.logits\n",
        "            predicted_labels = logits.argmax(dim=1)\n",
        "\n",
        "            predictions.extend(predicted_labels.tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision (Macro): {precision}\")\n",
        "    print(f\"Recall (Macro): {recall}\")\n",
        "    print(f\"F1 Score (Macro): {f1}\")"
      ],
      "metadata": {
        "id": "_Va-8dcgeWDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To test the model on the test data\n",
        "test(model, test_loader, device)"
      ],
      "metadata": {
        "id": "P5SdUE02eraD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT MUlti again"
      ],
      "metadata": {
        "id": "PTrey8zbn0f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "metadata": {
        "id": "lbiP11vbn4B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data\n",
        "train_df = dfs['train']\n",
        "test_df = dfs['test']\n",
        "val_df = dfs['validation']\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, num_categories):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.num_categories = num_categories\n",
        "        self.category_list = ['political', 'religion', 'gender', 'lgbtq', 'race']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment_text = self.data['comment_text'].iloc[index]\n",
        "        bias_label = 1 if self.data['bias'].iloc[index] == 'bias' else 0\n",
        "        category_label = [0] * self.num_categories\n",
        "\n",
        "        # Assuming the 'category' column is a comma-separated string of categories\n",
        "        categories = self.data['category'].iloc[index].split(', ')\n",
        "\n",
        "        # Set category_label to 1 for each category present in the comment\n",
        "        for category in categories:\n",
        "            if category in ['political', 'religion', 'gender', 'lgbtq', 'race']:\n",
        "                category_label[self.category_list.index(category)] = 1\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
        "            'bias_label': torch.tensor(bias_label, dtype=torch.float),\n",
        "            'category_labels': torch.tensor(category_label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Set the maximum sequence length for padding/truncation\n",
        "max_length = 128\n",
        "num_categories = 5  # Number of categories: Political, Religion, Gender, LGBTQ, Race\n",
        "\n",
        "# Assuming you have 'train_df', 'val_df', and 'test_df'\n",
        "train_dataset = CustomDataset(train_df, tokenizer, max_length, num_categories)\n",
        "val_dataset = CustomDataset(val_df, tokenizer, max_length, num_categories)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, max_length, num_categories)\n",
        "\n",
        "# Create data loaders for each dataset\n",
        "batch_size = 16  # You can adjust this based on your memory capacity\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "YYq1pQAo0arK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Initialize BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define your custom MTL model\n",
        "class MultiTaskBERT(nn.Module):\n",
        "    def __init__(self, num_categories):\n",
        "        super(MultiTaskBERT, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.bias_classifier = nn.Linear(768, 1)  # One neuron for bias detection\n",
        "        self.category_classifier = nn.Linear(768, num_categories)  # Five neurons for category classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # BERT forward pass\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs['pooler_output']\n",
        "\n",
        "        # Bias classification\n",
        "        bias_output = self.bias_classifier(pooled_output)\n",
        "\n",
        "        # Category classification\n",
        "        category_output = self.category_classifier(pooled_output)\n",
        "\n",
        "        return bias_output, category_output\n",
        "\n",
        "# Create an instance of the MTL model\n",
        "num_categories = 5  # Political, Religion, Gender, LGBTQ, Race\n",
        "model = MultiTaskBERT(num_categories)\n"
      ],
      "metadata": {
        "id": "R53tby1t0zO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias_criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss for bias detection\n",
        "category_criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss for category classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n"
      ],
      "metadata": {
        "id": "sg0rcxNz1A8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_data_loader:\n",
        "        input_ids, attention_mask, bias_labels, category_labels = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        bias_output, category_output = model(input_ids, attention_mask)\n",
        "\n",
        "        bias_loss = bias_criterion(bias_output, bias_labels)\n",
        "        category_loss = category_criterion(category_output, category_labels)\n",
        "\n",
        "        total_loss = bias_loss + category_loss\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "#     # Evaluate the model on the validation data\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         # Calculate validation loss and accuracy\n",
        "\n",
        "# # Test the model on the test data\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     # Calculate test loss and accuracy\n"
      ],
      "metadata": {
        "id": "N8Bwt4d41F37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fp5k5c_L1IHl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}